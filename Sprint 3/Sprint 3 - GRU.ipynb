{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1739eb9",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "058afe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 18:05:41.176724: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-07 18:05:41.750036: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-11-07 18:05:41.750095: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-07 18:05:41.816623: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-07 18:05:43.113192: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 18:05:43.113317: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-11-07 18:05:43.113328: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import datetime\n",
    "\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, GRU, Lambda\n",
    "import keras.backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb0a815",
   "metadata": {},
   "source": [
    "## Bases importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba41058e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5334/58716967.py:10: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_test = pd.read_csv(\"test.csv\").iloc[:500, :]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_base = pd.read_csv('./insurance_qna_dataset.csv',sep='\\t',index_col=0)\n",
    "questions_number = qa_base.shape[0]\n",
    "\n",
    "qm_base = pd.read_csv(\"Test questions dataset.csv\")\n",
    "test_questions_number = qm_base.shape[0]\n",
    "\n",
    "# Load training and test set\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\").iloc[:1000, :]\n",
    "df_test = pd.read_csv(\"test.csv\").iloc[:500, :]\n",
    "\n",
    "# qa_base.head()\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2125a33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  test_id                                          question1  \\\n",
       "0       0  How does the Surface Pro himself 4 compare wit...   \n",
       "1       1  Should I have a hair transplant at age 24? How...   \n",
       "2       2  What but is the best way to send money from Ch...   \n",
       "3       3                        Which food not emulsifiers?   \n",
       "4       4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \n",
       "0  Why did Microsoft choose core m3 and not core ...  \n",
       "1        How much cost does hair transplant require?  \n",
       "2                      What you send money to China?  \n",
       "3                                  What foods fibre?  \n",
       "4                     How their can I start reading?  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c6b1e5",
   "metadata": {},
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43e8eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "TRAIN_CSV = \"./test.csv\"\n",
    "TEST_CSV = \"./test.csv\"\n",
    "MODEL_SAVING_DIR = \"./Sprint 3/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492ab99",
   "metadata": {},
   "source": [
    "## Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1098f270",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "def text_to_word_list(text):\n",
    "    ''' Pre process and convert texts to a list of words '''\n",
    "    text = str(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "\n",
    "    text = text.split()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Prepare embedding\n",
    "vocabulary = dict()\n",
    "inverse_vocabulary = []  # '' will never be used, it is only a placeholder for the [0, 0, ....0] embedding\n",
    "\n",
    "questions_cols = ['question1', 'question2']\n",
    "\n",
    "# Iterate over the questions only of both training and test datasets\n",
    "for dataset in [df_train, df_test]:\n",
    "    for index, row in dataset.iterrows():\n",
    "\n",
    "        # Iterate through the text of both questions of the row\n",
    "        for question in questions_cols:\n",
    "\n",
    "            q2n = []  # q2n -> question numbers representation\n",
    "            for word in text_to_word_list(row[question]):\n",
    "\n",
    "                # Check for unwanted words\n",
    "\n",
    "                if word not in vocabulary:\n",
    "                    vocabulary[word] = len(inverse_vocabulary)\n",
    "                    q2n.append(len(inverse_vocabulary))\n",
    "                    inverse_vocabulary.append(word)\n",
    "                else:\n",
    "                    q2n.append(vocabulary[word])\n",
    "\n",
    "            # Replace questions as word to question as number representation\n",
    "            dataset.at[index, question] =  q2n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4959d",
   "metadata": {},
   "source": [
    "## Prepare training and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79fd45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left': 331    [0, 118, 2, 1585, 66, 1586, 13, 2, 1851, 1852,...\n",
      "875    [17, 625, 6, 1224, 3603, 8, 2688, 66, 2703, 17...\n",
      "487    [0, 1, 2477, 2478, 66, 43, 100, 338, 347, 2477...\n",
      "457           [0, 118, 2, 601, 700, 80, 1220, 2362, 150]\n",
      "295    [0, 167, 38, 2, 1716, 13, 1717, 1253, 66, 1104...\n",
      "                             ...                        \n",
      "88              [58, 1, 2, 198, 672, 673, 674, 675, 676]\n",
      "390    [0, 1, 2, 552, 412, 822, 2085, 2086, 66, 2087,...\n",
      "958    [27, 3821, 28, 17, 1153, 8, 3822, 1534, 164, 3...\n",
      "120               [43, 31, 216, 137, 855, 539, 856, 857]\n",
      "183                    [27, 100, 17, 50, 36, 1182, 1183]\n",
      "Name: question1, Length: 900, dtype: object, 'right': 331    [0, 118, 2, 1585, 66, 1586, 13, 1089, 36, 1853...\n",
      "875    [17, 44, 3607, 66, 17, 176, 6, 7, 254, 8, 988,...\n",
      "487                           [0, 1, 2477, 2478, 8, 899]\n",
      "457    [0, 118, 2, 601, 700, 102, 28, 136, 94, 2, 502...\n",
      "295    [0, 167, 38, 2, 1716, 13, 2, 1719, 13, 2, 1720...\n",
      "                             ...                        \n",
      "88              [58, 1, 2, 198, 672, 673, 674, 677, 675]\n",
      "390       [0, 1, 2, 667, 412, 822, 66, 2087, 2088, 2085]\n",
      "958    [28, 17, 1153, 3821, 8, 1534, 164, 3823, 374, ...\n",
      "120    [27, 28, 17, 196, 36, 216, 497, 277, 855, 539,...\n",
      "183             [27, 100, 1182, 1184, 283, 102, 2, 1185]\n",
      "Name: question2, Length: 900, dtype: object}\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = max(df_train.question1.map(lambda x: len(x)).max(),\n",
    "                     df_train.question2.map(lambda x: len(x)).max(),\n",
    "                     df_test.question1.map(lambda x: len(x)).max(),\n",
    "                     df_test.question2.map(lambda x: len(x)).max())\n",
    "\n",
    "X = df_train[questions_cols]\n",
    "Y = df_train['is_duplicate']\n",
    "\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=0.1)\n",
    "\n",
    "# Split to dicts\n",
    "X_train = {'left': X_train.question1, 'right': X_train.question2}\n",
    "X_validation = {'left': X_validation.question1, 'right': X_validation.question2}\n",
    "X_test = {'left': df_test.question1, 'right': df_test.question2}\n",
    "\n",
    "# Convert labels to their numpy representations\n",
    "Y_train = Y_train.values\n",
    "Y_validation = Y_validation.values\n",
    "\n",
    "print(X_train)\n",
    "# Zero padding\n",
    "for dataset, side in itertools.product([X_train, X_validation], ['left', 'right']):\n",
    "    dataset[side] = pad_sequences(dataset[side], maxlen=max_seq_length)\n",
    "\n",
    "    \n",
    "# Make sure everything is ok\n",
    "assert X_train['left'].shape == X_train['right'].shape\n",
    "assert len(X_train['left']) == len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4af663",
   "metadata": {},
   "source": [
    "## Contrastive loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d19774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(margin):\n",
    "    \"\"\"Provides 'constrastive_loss' an enclosing scope with variable 'margin'.\n",
    "\n",
    "    Arguments:\n",
    "        margin: Integer, defines the baseline for distance for which pairs\n",
    "                should be classified as dissimilar. - (default is 1).\n",
    "\n",
    "    Returns:\n",
    "        'constrastive_loss' function with data ('margin') attached.\n",
    "    \"\"\"\n",
    "\n",
    "    # Contrastive loss = mean( (1-true_value) * square(prediction) +\n",
    "    #                         true_value * square( max(margin-prediction, 0) ))\n",
    "    def contrastive_loss(y_true, y_pred):\n",
    "        \"\"\"Calculates the constrastive loss.\n",
    "\n",
    "        Arguments:\n",
    "            y_true: List of labels, each label is of type float32.\n",
    "            y_pred: List of predictions of same length as of y_true,\n",
    "                    each label is of type float32.\n",
    "\n",
    "        Returns:\n",
    "            A tensor containing constrastive loss as floating point value.\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        #print(y_true)\n",
    "        #print(y_pred)\n",
    "        \n",
    "        square_pred = tf.math.square(y_pred)\n",
    "        margin_square = tf.math.square(tf.math.maximum(margin - (y_pred), 0))\n",
    "        return tf.math.reduce_mean(\n",
    "            (1 - y_true) * square_pred + (y_true) * margin_square\n",
    "        )\n",
    "\n",
    "    return contrastive_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020e1c0",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a948c516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0    0    0 ... 1851 1852 1021]\n",
      " [   0    0    0 ...   17   44 3606]\n",
      " [   0    0    0 ... 2478    8  899]\n",
      " ...\n",
      " [   0    0    0 ... 3823  374  643]\n",
      " [   0    0    0 ...  539  856  857]\n",
      " [   0    0    0 ...   36 1182 1183]]\n",
      "[[   0    0    0 ... 1853 1851 1852]\n",
      " [   0    0    0 ...   87   17 1224]\n",
      " [   0    0    0 ... 2478    8  899]\n",
      " ...\n",
      " [   0    0    0 ... 3824  374  643]\n",
      " [   0    0    0 ...  858    6  320]\n",
      " [   0    0    0 ...  102    2 1185]]\n"
     ]
    }
   ],
   "source": [
    "# Model variables\n",
    "n_hidden = 50\n",
    "gradient_clipping_norm = 1.25\n",
    "batch_size = 64\n",
    "n_epoch = 10\n",
    "\n",
    "def exponent_neg_manhattan_distance(left, right):\n",
    "    ''' Helper function for the similarity estimate of the LSTMs outputs'''\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n",
    "\n",
    "# The visible layer\n",
    "left_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "right_input = Input(shape=(max_seq_length,), dtype='int32')\n",
    "\n",
    "embedding_dim = 25\n",
    "\n",
    "embedding_layer = Embedding(len(vocabulary), embedding_dim, input_length=max_seq_length) #len(embeddings) = len(vocabulary)\n",
    "\n",
    "# Embedded version of the inputs\n",
    "encoded_left = embedding_layer(left_input)\n",
    "encoded_right = embedding_layer(right_input)\n",
    "\n",
    "# Since this is a siamese network, both sides share the same LSTM\n",
    "shared_gru = GRU(n_hidden)\n",
    "\n",
    "left_output = shared_gru(encoded_left)\n",
    "right_output = shared_gru(encoded_right)\n",
    "\n",
    "# Calculates the distance as defined by the MaLSTM model\n",
    "magru_distance = Lambda(function=lambda x: exponent_neg_manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_output, right_output])\n",
    "\n",
    "# Pack it all up into a model\n",
    "magru = Model([left_input, right_input], [magru_distance])\n",
    "\n",
    "# Adadelta optimizer, with gradient clipping by norm\n",
    "optimizer = Adadelta(clipnorm=gradient_clipping_norm)\n",
    "\n",
    "magru.compile(loss='mean_squared_error', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "#callback checkpoint (save best model)\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='./igor/checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "# Start training\n",
    "training_start_time = time()\n",
    "\n",
    "# magru_trained = magru.fit([X_train['left'], X_train['right']], Y_train, batch_size=batch_size, epochs=n_epoch,\n",
    "#                             validation_data=([X_validation['left'], X_validation['right']], Y_validation), callbacks=[model_checkpoint_callback])\n",
    "\n",
    "print(X_train['left'])\n",
    "print(X_train['right'])\n",
    "\n",
    "# print(\"Training time finished.\\n{} epochs in {}\".format(n_epoch, datetime.timedelta(seconds=time()-training_start_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b9b09",
   "metadata": {},
   "source": [
    "## Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41be9d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "\n",
    "plt.plot(malstm_trained.history['accuracy'])\n",
    "\n",
    "plt.plot(malstm_trained.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot loss\n",
    "plt.plot(malstm_trained.history['loss'])\n",
    "plt.plot(malstm_trained.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
